{"nbformat": 4, "nbformat_minor": 5, "metadata": {"colab": {"provenance": []}, "kernelspec": {"name": "python3", "language": "python", "display_name": "Python 3"}, "language_info": {"name": "python", "pygments_lexer": "ipython3"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# AP Study Assistant — Bilingual Notes & Mind Map (Colab)\n", "\n", "本 Notebook 版**可直接在 Colab 运行**：从输入文本生成**中英对照笔记**（英文原句 + 中文术语注释 gloss）、**Mermaid 思维导图**，并输出**词频图表**与可直接上传到 GitHub 的项目压缩包。\n", "\n", "### 你能得到\n", "- `output/notes_bilingual.md`：中英对照（演示：中文为术语注释）。\n", "- `output/mindmap.mmd`：Mermaid 思维导图（GitHub 可直接渲染）。\n", "- `assets/wordfreq.png`：词频柱状图（效果图）。\n", "- `assets/notes_preview.png`：笔记预览图（效果图）。\n", "- `ap-note-gen.zip`：含 README、源码、assets、sample 和 output 的**整站压缩包**，可直接传 GitHub。\n", "\n", "> 默认不需要外网依赖；如需“真正的机器翻译”和“PDF 文本提取”，可在后面的**可选进阶**单元中安装 `transformers` / `pymupdf`。\n"]}, {"cell_type": "code", "metadata": {}, "source": ["#@title 1) 基础导入 & 目录准备（必须先运行）\n", "from pathlib import Path\n", "import re, os\n", "from collections import Counter\n", "import matplotlib.pyplot as plt\n", "\n", "ROOT = Path('.')\n", "ASSETS = ROOT / 'assets'\n", "OUTPUT = ROOT / 'output'\n", "SRC = ROOT / 'src' / 'ap_note_gen'\n", "SAMPLE = ROOT / 'sample_data'\n", "for p in [ASSETS, OUTPUT, SRC, SAMPLE]:\n", "    p.mkdir(parents=True, exist_ok=True)\n", "print('Dirs ready:', ASSETS, OUTPUT, SRC, SAMPLE)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["#@title 2) 核心函数：分句、摘要、术语注释、导出\n", "STOP = set('''a an the and or but if while to for of in on by with from as about across after against among around at before behind below beneath beside between beyond during except inside into like near off onto outside over past since through toward under until up upon within without is are was were be been being this that these those not no nor so such than then too very can could should would may might must do does did having have has it its it's they them he she we you i our your their'''.split())\n", "\n", "def simple_sentence_split(text: str):\n", "    text = re.sub(r\"\\s+\", \" \", text.strip())\n", "    parts = re.split(r\"(?<=[\\.!\\?])\\s+\", text)\n", "    return [s.strip() for s in parts if s.strip()]\n", "\n", "def tokenize(text: str):\n", "    text = text.lower()\n", "    text = re.sub(r\"[^a-z0-9\\s\\-]\", \" \", text)\n", "    return [t for t in text.split() if t]\n", "\n", "def score_sentences(sentences):\n", "    tokens = []\n", "    for s in sentences:\n", "        tokens.extend([t for t in tokenize(s) if t not in STOP])\n", "    c = Counter(tokens)\n", "    if not c:\n", "        return [0.0]*len(sentences)\n", "    maxf = max(c.values())\n", "    for k in list(c.keys()):\n", "        c[k] = c[k]/maxf\n", "    scores = []\n", "    for s in sentences:\n", "        ts = [t for t in tokenize(s) if t not in STOP]\n", "        score = sum(c.get(t,0) for t in ts) / (len(ts)+1e-9)\n", "        scores.append(score)\n", "    return scores\n", "\n", "def top_k_summary(sentences, k=6):\n", "    if not sentences:\n", "        return []\n", "    scores = score_sentences(sentences)\n", "    idx = list(range(len(sentences)))\n", "    idx.sort(key=lambda i: scores[i], reverse=True)\n", "    chosen = sorted(idx[:k])\n", "    return [sentences[i] for i in chosen]\n", "\n", "CN_GLOSS = {\n", "    \"experiment\":\"实验\",\"experimental\":\"实验的\",\"design\":\"设计\",\"random\":\"随机\",\n", "    \"randomized\":\"随机化\",\"assignment\":\"分配\",\"comparative\":\"对比\",\"control\":\"控制\",\n", "    \"confounding\":\"混杂\",\"confounders\":\"混杂因素\",\"variable\":\"变量\",\"variables\":\"变量\",\n", "    \"response\":\"应变量\",\"explanatory\":\"自变量\",\"blocking\":\"分组(区组)\",\"block\":\"区组\",\n", "    \"matched\":\"匹配\",\"pairs\":\"配对\",\"replication\":\"重复(样本量)\",\"bias\":\"偏差\",\n", "    \"blinding\":\"盲法\",\"double-blind\":\"双盲\",\"single-blind\":\"单盲\",\"placebo\":\"安慰剂\",\n", "    \"precision\":\"精度\",\"variability\":\"变异\",\"population\":\"总体\",\"sample\":\"样本\",\n", "    \"generalization\":\"外推(泛化)\",\"causal\":\"因果的\",\"inference\":\"推断\",\"scope\":\"适用范围\"\n", "}\n", "\n", "def gloss_translate(en_sentence: str) -> str:\n", "    words = en_sentence.split()\n", "    out = []\n", "    for w in words:\n", "        base = re.sub(r\"[^a-zA-Z\\-]\", \"\", w).lower()\n", "        cn = CN_GLOSS.get(base)\n", "        out.append(f\"{w}({cn})\" if cn else w)\n", "    return \" \".join(out)\n", "\n", "def build_bilingual_notes(summary_sentences):\n", "    notes = []\n", "    for s in summary_sentences:\n", "        zh = gloss_translate(s)\n", "        notes.append({\"en\": s, \"zh\": zh})\n", "    return notes\n", "\n", "def export_markdown(notes, out_path: str):\n", "    md = [\"# AP Bilingual Notes (Auto)\",\n", "          \"\",\n", "          \"> 本文件由脚本自动生成：上为原文句子，下为带中文术语提示的对照（示范模式）。\",\n", "          \"\"]\n", "    for i, item in enumerate(notes, 1):\n", "        md.append(f\"## {i}.\")\n", "        md.append(\"**EN**: \" + item[\"en\"])\n", "        md.append(\"**ZH(Gloss)**: \" + item[\"zh\"])\n", "        md.append(\"\")\n", "    Path(out_path).write_text(\"\\n\".join(md), encoding=\"utf-8\")\n", "\n", "def export_mermaid_mindmap(notes, out_path: str, title=\"AP Experimental Design\"):\n", "    all_text = \" \".join(n[\"en\"] for n in notes).lower()\n", "    tokens = [t for t in re.findall(r\"[a-z\\-]+\", all_text) if t not in STOP]\n", "    c = Counter(tokens)\n", "    core = [w for w,_ in c.most_common(10)]\n", "    lines = [\"```mermaid\",\"mindmap\",f\"  root(({title}))\"]\n", "    for w in core:\n", "        lines.append(f\"    {w}\")\n", "    lines.append(\"```\")\n", "    Path(out_path).write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n", "\n", "def word_frequencies(text: str, topn=12):\n", "    tokens = [t for t in re.findall(r\"[a-zA-Z\\-]+\", text.lower()) if t not in STOP]\n", "    c = Counter(tokens)\n", "    return c.most_common(topn)\n"]}, {"cell_type": "code", "metadata": {}, "source": ["#@title 3) 示例文本（可替换为你自己的 .txt 文本）\n", "sample_text = (\n", "    \"\"\"\n", "AP Statistics – Experimental Design (author-made demo text)\n", "\n", "In AP Statistics, experimental design focuses on how to gather convincing evidence about cause-and-effect.\n", "Key ideas include: explanatory vs. response variables, control of confounding, random assignment, and replication.\n", "\n", "A good randomized comparative experiment assigns subjects to treatments by chance. Random assignment helps balance\n", "unknown confounders between groups, so differences in responses can more credibly be attributed to the treatments.\n", "Controlling conditions (like environment or instructions) reduces variability unrelated to the treatments.\n", "\n", "Blocking groups similar subjects together on a known factor (e.g., prior achievement) can increase precision by\n", "reducing within-block variability. Matched-pairs designs are a special case of blocking, where each pair is closely\n", "matched (or each subject serves as their own pair across two conditions). Replication—using enough subjects—stabilizes\n", "estimates and makes results more generalizable.\n", "\n", "Blinding helps mitigate bias: single-blind means participants do not know which treatment they receive; double-blind\n", "means both participants and those measuring responses do not know the treatment assignments. Placebo controls help\n", "separate the true treatment effect from psychological or expectation effects.\n", "\n", "Finally, scope of inference depends on the design: random assignment supports causal inference; random sampling supports\n", "population generalization. An experiment with random assignment but convenience sampling allows strong causal claims for\n", "the studied subjects, but generalization to a wider population remains limited.\n", "    \"\"\".strip()\n", ")\n", "\n", "(SAMPLE / 'demo_experimental_design.txt').write_text(sample_text, encoding='utf-8')\n", "print('Sample saved to', SAMPLE / 'demo_experimental_design.txt')\n"]}, {"cell_type": "code", "metadata": {}, "source": ["#@title 4) 运行管线（生成笔记 + 思维导图 + 词频图）\n", "text = (SAMPLE / 'demo_experimental_design.txt').read_text(encoding='utf-8')\n", "sentences = simple_sentence_split(text)\n", "summary = top_k_summary(sentences, k=6)\n", "notes = build_bilingual_notes(summary)\n", "\n", "export_markdown(notes, str(OUTPUT / 'notes_bilingual.md'))\n", "export_mermaid_mindmap(notes, str(OUTPUT / 'mindmap.mmd'), title='AP Experimental Design')\n", "\n", "# 词频图\n", "wf = word_frequencies(text, topn=12)\n", "words, counts = zip(*wf) if wf else ([],[])\n", "plt.figure(figsize=(8,4.5))\n", "plt.bar(range(len(words)), counts)\n", "plt.xticks(range(len(words)), words, rotation=30, ha='right')\n", "plt.title('Word Frequency (Top 12) – Demo')\n", "plt.tight_layout()\n", "plt.savefig(ASSETS / 'wordfreq.png')\n", "plt.show()\n", "\n", "# 生成笔记预览图\n", "preview_lines = (OUTPUT / 'notes_bilingual.md').read_text(encoding='utf-8').splitlines()[:22]\n", "plt.figure(figsize=(9,6))\n", "plt.axis('off')\n", "plt.text(0.01, 0.98, \"\\n\".join(preview_lines), va='top', ha='left', wrap=True, fontsize=9, family='monospace')\n", "plt.tight_layout()\n", "plt.savefig(ASSETS / 'notes_preview.png')\n", "plt.show()\n", "\n", "print('Generated files:', list(OUTPUT.iterdir()))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### （可选）进阶：真实翻译 / PDF 文本提取\n", "\n", "如果你需要**真实机器翻译**或直接从 **PDF** 提取文本：\n", "\n", "```python\n", "# 安装\n", "!pip -q install transformers sentencepiece\n", "!pip -q install pymupdf  # 可选：PDF 文本提取\n", "\n", "from transformers import MarianTokenizer, MarianMTModel\n", "tok = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-zh')\n", "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-zh')\n", "\n", "def true_translate(en_sentence):\n", "    inputs = tok([en_sentence], return_tensors='pt', padding=True)\n", "    out = model.generate(**inputs, max_new_tokens=128)\n", "    return tok.batch_decode(out, skip_special_tokens=True)[0]\n", "```\n", "\n", "PDF 提取（用 `pymupdf`）示例：\n", "\n", "```python\n", "import fitz  # pymupdf\n", "def pdf_to_text(path):\n", "    doc = fitz.open(path)\n", "    texts = []\n", "    for page in doc:\n", "        texts.append(page.get_text())\n", "    return \"\\n\".join(texts)\n", "```\n"]}, {"cell_type": "code", "metadata": {}, "source": ["#@title 5) 一键导出为可传 GitHub 的压缩包（含 README、源码、assets、sample、output）\n", "readme = f\"\"\"\n", "# AP Study Assistant — Bilingual Notes & Mind Map Generator (Colab Export)\n", "\n", "> 一键生成 **中英文对照笔记** + **Mermaid 思维导图**（GitHub 可直接渲染）。本导出包来自 Colab 运行结果，包含示例与效果图。\n", "\n", "## 功能\n", "- 句子级提取摘要（轻量频次模型）。\n", "- 自动输出**中英对照**（英文原句 + 中文术语提示 gloss）。\n", "- 输出 **Mermaid 思维导图**（`output/mindmap.mmd`）。\n", "- 输出**词频统计**与效果图：`assets/wordfreq.png`、`assets/notes_preview.png`。\n", "\n", "## 创意点\n", "1. 面向 AP 学科的**术语敏感**笔记生成，不是单纯机翻。\n", "2. 使用 Mermaid 生成轻量思维导图，便于版本管理。\n", "3. 结构清晰，便于扩展接入更强的翻译与摘要模型。\n", "\n", "## 快速开始\n", "```bash\n", "python -m src.ap_note_gen.cli --input sample_data/demo_experimental_design.txt --outdir output --k 6\n", "```\n", "\n", "## 待改进\n", "- 扩充术语词典；引入 transformers 做真实翻译。\n", "- 支持 PDF/批量处理/前端上传。\n", "\n", "## 效果图\n", "![Word Frequency](assets/wordfreq.png)\n", "\n", "笔记预览：\n", "![Notes Preview](assets/notes_preview.png)\n", "        \n", "## 目录结构\n", "```\n", ".\n", "├── README.md\n", "├── assets/\n", "│   ├── wordfreq.png\n", "│   └── notes_preview.png\n", "├── output/\n", "│   ├── notes_bilingual.md\n", "│   └── mindmap.mmd\n", "├── sample_data/\n", "│   └── demo_experimental_design.txt\n", "└── src/\n", "    └── ap_note_gen/\n", "        ├── __init__.py\n", "        ├── pipeline.py\n", "        └── cli.py\n", "```\n", "        \n", "## 许可\n", "MIT\n", "        \n", "\"\"\"\n", "\n", "# 写出源码（与 Notebook 同步的轻量版本）\n", "(SRC / '__init__.py').write_text(\"__version__='0.1.0'\\n\", encoding='utf-8')\n", "\n", "pipeline_py = r'''\n", "import re\n", "from pathlib import Path\n", "from collections import Counter\n", "\n", "STOP = set('''a an the and or but if while to for of in on by with from as about across after against among around at before behind below beneath beside between beyond during except inside into like near off onto outside over past since through toward under until up upon within without is are was were be been being this that these those not no nor so such than then too very can could should would may might must do does did having have has it its it's they them he she we you i our your their'''.split())\n", "\n", "def simple_sentence_split(text: str):\n", "    text = re.sub(r\"\\s+\", \" \", text.strip())\n", "    parts = re.split(r\"(?<=[\\.!\\?])\\s+\", text)\n", "    return [s.strip() for s in parts if s.strip()]\n", "\n", "def tokenize(text: str):\n", "    text = text.lower()\n", "    text = re.sub(r\"[^a-z0-9\\s\\-]\", \" \", text)\n", "    return [t for t in text.split() if t]\n", "\n", "def score_sentences(sentences):\n", "    tokens = []\n", "    for s in sentences:\n", "        tokens.extend([t for t in tokenize(s) if t not in STOP])\n", "    c = Counter(tokens)\n", "    if not c:\n", "        return [0.0]*len(sentences)\n", "    maxf = max(c.values())\n", "    for k in list(c.keys()):\n", "        c[k] = c[k]/maxf\n", "    scores = []\n", "    for s in sentences:\n", "        ts = [t for t in tokenize(s) if t not in STOP]\n", "        score = sum(c.get(t,0) for t in ts) / (len(ts)+1e-9)\n", "        scores.append(score)\n", "    return scores\n", "\n", "def top_k_summary(sentences, k=6):\n", "    if not sentences:\n", "        return []\n", "    scores = score_sentences(sentences)\n", "    idx = list(range(len(sentences)))\n", "    idx.sort(key=lambda i: scores[i], reverse=True)\n", "    chosen = sorted(idx[:k])\n", "    return [sentences[i] for i in chosen]\n", "\n", "CN_GLOSS = {\n", "    \"experiment\":\"实验\",\"experimental\":\"实验的\",\"design\":\"设计\",\"random\":\"随机\",\n", "    \"randomized\":\"随机化\",\"assignment\":\"分配\",\"comparative\":\"对比\",\"control\":\"控制\",\n", "    \"confounding\":\"混杂\",\"confounders\":\"混杂因素\",\"variable\":\"变量\",\"variables\":\"变量\",\n", "    \"response\":\"应变量\",\"explanatory\":\"自变量\",\"blocking\":\"分组(区组)\",\"block\":\"区组\",\n", "    \"matched\":\"匹配\",\"pairs\":\"配对\",\"replication\":\"重复(样本量)\",\"bias\":\"偏差\",\n", "    \"blinding\":\"盲法\",\"double-blind\":\"双盲\",\"single-blind\":\"单盲\",\"placebo\":\"安慰剂\",\n", "    \"precision\":\"精度\",\"variability\":\"变异\",\"population\":\"总体\",\"sample\":\"样本\",\n", "    \"generalization\":\"外推(泛化)\",\"causal\":\"因果的\",\"inference\":\"推断\",\"scope\":\"适用范围\"\n", "}\n", "\n", "def gloss_translate(en_sentence: str) -> str:\n", "    words = en_sentence.split()\n", "    out = []\n", "    for w in words:\n", "        base = re.sub(r\"[^a-zA-Z\\-]\", \"\", w).lower()\n", "        cn = CN_GLOSS.get(base)\n", "        out.append(f\"{w}({cn})\" if cn else w)\n", "    return \" \".join(out)\n", "\n", "def build_bilingual_notes(summary_sentences):\n", "    notes = []\n", "    for s in summary_sentences:\n", "        zh = gloss_translate(s)\n", "        notes.append({\"en\": s, \"zh\": zh})\n", "    return notes\n", "\n", "def export_markdown(notes, out_path: str):\n", "    md = [\"# AP Bilingual Notes (Auto)\",\n", "          \"\",\n", "          \"> 自动生成：上为原文句子，下为带中文术语提示的对照（示范模式）。\",\n", "          \"\"]\n", "    for i, item in enumerate(notes, 1):\n", "        md.append(f\"## {i}.\")\n", "        md.append(\"**EN**: \" + item[\"en\"])\n", "        md.append(\"**ZH(Gloss)**: \" + item[\"zh\"])\n", "        md.append(\"\")\n", "    Path(out_path).write_text(\"\\n\".join(md), encoding=\"utf-8\")\n", "\n", "def export_mermaid_mindmap(notes, out_path: str, title=\"AP Experimental Design\"):\n", "    all_text = \" \".join(n[\"en\"] for n in notes).lower()\n", "    tokens = [t for t in re.findall(r\"[a-z\\-]+\", all_text) if t not in STOP]\n", "    c = Counter(tokens)\n", "    core = [w for w,_ in c.most_common(10)]\n", "    lines = [\"```mermaid\",\"mindmap\",f\"  root(({title}))\"]\n", "    for w in core:\n", "        lines.append(f\"    {w}\")\n", "    lines.append(\"```\")\n", "    Path(out_path).write_text(\"\\n\".join(lines), encoding=\"utf-8\")\n", "'''\n", "(SRC / 'pipeline.py').write_text(pipeline_py, encoding='utf-8')\n", "\n", "cli_py = r'''\n", "import argparse\n", "from pathlib import Path\n", "from .pipeline import simple_sentence_split, top_k_summary, build_bilingual_notes, export_markdown, export_mermaid_mindmap\n", "\n", "def main():\n", "    ap = argparse.ArgumentParser(description='AP bilingual note generator (demo)')\n", "    ap.add_argument('--input', required=True, help='Path to plain .txt')\n", "    ap.add_argument('--outdir', default='output', help='Directory to save outputs')\n", "    ap.add_argument('--k', type=int, default=6, help='Number of summary sentences')\n", "    args = ap.parse_args()\n", "    Path(args.outdir).mkdir(parents=True, exist_ok=True)\n", "    text = Path(args.input).read_text(encoding='utf-8', errors='ignore')\n", "    sentences = simple_sentence_split(text)\n", "    summary = top_k_summary(sentences, k=args.k)\n", "    notes = build_bilingual_notes(summary)\n", "    export_markdown(notes, str(Path(args.outdir)/'notes_bilingual.md'))\n", "    export_mermaid_mindmap(notes, str(Path(args.outdir)/'mindmap.mmd'))\n", "    print('Saved to', args.outdir)\n", "if __name__ == '__main__':\n", "    main()\n", "'''\n", "(SRC / 'cli.py').write_text(cli_py, encoding='utf-8')\n", "\n", "# 写 README\n", "Path('README.md').write_text(readme, encoding='utf-8')\n", "\n", "# 拷贝运行结果与示例\n", "from shutil import copyfile\n", "copyfile(ASSETS / 'wordfreq.png', ASSETS / 'wordfreq.png')\n", "copyfile(ASSETS / 'notes_preview.png', ASSETS / 'notes_preview.png')\n", "\n", "# 压缩\n", "zip_path = Path('ap-note-gen.zip')\n", "import zipfile\n", "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as z:\n", "    for folder, _, files in os.walk('.'):\n", "        # 排除 ./.ipynb_checkpoints 和本 ipynb 本身\n", "        if '.ipynb_checkpoints' in folder:\n", "            continue\n", "        for f in files:\n", "            if f.endswith('.ipynb'):\n", "                continue\n", "            fp = Path(folder) / f\n", "            z.write(fp, fp.relative_to('.'))\n", "print('Exported zip ->', zip_path.resolve())\n"]}]}